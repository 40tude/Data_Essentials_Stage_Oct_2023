{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing\n",
    "\n",
    "One of the use cases of Hypothesis Testing are A/B tests. As it is widely used in Data Analytics, let's focus on this specific application. In this course we will first explain the fundamentals of AB testing and then we will create experiments on Python.\n",
    "\n",
    "## What you will learn in this course 🧐🧐\n",
    "\n",
    "- What's an A/B test\n",
    "- Important concepts to consider before running an A/B test\n",
    "- How to create an A/B test\n",
    "- The limits of the A/B test\n",
    "\n",
    "## What's an A/B test?\n",
    "\n",
    "## Definition\n",
    "\n",
    "#### According to Optimizely\n",
    "\n",
    "Optimizely is an American startup pioneer in A/B test automation. Here is their definition of the method:\n",
    "\n",
    "> A/B testing (also known as [split testing](https://www.optimizely.com/split-testing/) or [bucket testing](https://www.optimizely.com/optimization-glossary/bucket-testing/)) is a method of comparing two versions of a webpage or app against each other to determine which one performs better. AB testing is essentially an experiment where two or more variants of a page are shown to users at random, and statistical analysis is used to determine which variation performs better for a given conversion goal.\n",
    "\n",
    "An A/B is therefore to compare two versions of your web page and see which one performs better. What is important to remember is that an A/B test is based on statistical data to determine whether your results are actually correct in reality or whether they have been biased by external factors.\n",
    "\n",
    "It is also important to keep in mind that both versions of the A/B test are launched at the same time. Otherwise, it is a sequential test with no statistical value.\n",
    "\n",
    "#### Control Variable vs. Variation\n",
    "\n",
    "Version A of a site is often referred to as the control variable and version B as the variation. Keeping a control variable is extremely important because it allows you to avoid biasing your experience with external factors.\n",
    "\n",
    "Let's take an example. You make the assumption that changing the shape of your button on your home page will bring you more conversions. You don't run an A/B test, you simply change your site and observe the result. Two weeks later, your sales have increased by 30%. However, you have just entered the Christmas period and your business is extremely sensitive to seasonality. Therefore, you can't know if the button change had any effect on your conversions because of this external factor.\n",
    "\n",
    "## Why A/B testing?\n",
    "\n",
    "### Your instincts are often wrong\n",
    "\n",
    "Most of the time, the assumptions you're going to make will be wrong. Indeed, it is very difficult to know what customers want (especially for large companies where employees are quite far away from the customer's purchasing process). In addition, there is a lot of bias and conflicting interests. For example, a designer is likely to think that a visual and innovative website will lead to more conversions. However, if your customers are accustomed to standard websites, having a website that doesn't respect the codes will have a negative impact on your conversions.\n",
    "\n",
    "The AB test will allow you to create a mathematical model that can validate or invalidate your assumptions and thus implement the right improvements.\n",
    "\n",
    "### Prove causality\n",
    "\n",
    "You have to make the difference between correlation and causality. Correlation explains that if an event X occurs then something will happen to the factor Y, but we don't understand why this happens. The A/B test will allow you to prove the cause of this correlation and thus create a causal relationship. In other words, causality explains that if Y goes up it is because X has undergone this particular change. Causality is therefore more powerful than a simple correlation.\n",
    "\n",
    "### The limits of A/B testing\n",
    "\n",
    "Before doing an A/B test, there are several points you should consider.\n",
    "\n",
    "#### You need users\n",
    "\n",
    "An A/B test requires a large number of users. Indeed, if you wish to have statistically significant results, you will need a certain number of users to convert on your site. If, in general, you have relatively few users who convert, then you should avoid doing an A/B test because you will spend too much time waiting for the experience to end.\n",
    "\n",
    "#### It can take a long time\n",
    "\n",
    "As explained earlier, if you have few conversions, you will have to wait a long time before you have significant results. In addition, it will take some time for your experience to stabilize. This is because you may have large fluctuations that will bias the experiment, so you will have to wait for the statistics to balance out before you get significant results. In any case, you will need to wait at least a week before expecting statistical results.\n",
    "\n",
    "#### There is nothing else you can do during an experiment.\n",
    "\n",
    "Since you are testing two variations of your site, you cannot change anything else on it before the end of your experiment, otherwise it will bias the experiment. More seriously, this means that if you see a bug on your site, you won't be able to fix it before the end of your experiment or you will have to start over from the beginning.\n",
    "\n",
    "## The A/B test cycle\n",
    "\n",
    "To create an A/B test, you need to follow these simple steps that will help you prepare, run and interpret the results of your experiment.\n",
    "\n",
    "### Analyze\n",
    "\n",
    "#### Assess your data\n",
    "\n",
    "The first step is to analyze the data you already have and see where you can start your optimizations. Use the data in Google Analytics to get started. The behavioral feed will allow you to identify the number of people leaving your site before they finish their payment and exit page. This data can then be used to start your optimizations.\n",
    "\n",
    "#### Know your goals\n",
    "\n",
    "Once you understand your data, you need to define and prioritize your objectives. For example, you will probably want to optimize your newsletter subscription rate before optimizing the reading time of your home page. Once you have a goal in place, it will be very simple for you to decide which variation is a winner or a loser.\n",
    "\n",
    "### Create your hypotheses\n",
    "\n",
    "Now that you've identified the potential variations, you'll need to make a hypothesis. For example, you will hypothesize that a green button will convert more than a red button because you know that the color red is synonymous with danger. Here's what you need to keep in mind as you formulate your hypothesis.\n",
    "\n",
    "#### Testability\n",
    "\n",
    "A good hypothesis must be testable. This seems obvious and yet it is easy to fall into this trap especially because of the time. If your hypothesis takes too long to be tested, then it is simply not testable. Very often small companies want to implement an A/B test to improve their sales by changing the design of their website. However, if the company makes 5 sales a day, it will take months before they have a large enough sample of sales to get significant results.\n",
    "\n",
    "A good way to get around this problem is to try to improve your micro-conversions. You can, for example, try to measure the number of clicks on a button instead of a sale. By extension, you may conclude that the more people who click, the more likely you are to make a sale.\n",
    "\n",
    "#### Clear KPIs\n",
    "\n",
    "You need to be sure that your A/B test will have an impact on your most important KPIs. You can't just say \"I want to improve the engagement of my site\". You will need to go deeper and see what metrics (KPIs) you need to identify to improve your site's engagement. This can be the number of clicks, time spent on the site, number of shares etc. It is up to you to choose which of these metrics is the most relevant.\n",
    "\n",
    "#### Data on your market\n",
    "\n",
    "This last point is optional because it is much harder to obtain, but a hypothesis that also gives you knowledge about your market is a very good hypothesis. For example, you make the assumption that your users are very sensitive to the price of your product, so you can experiment by changing the price of your product from 10€ to 9.99€. If you see a large increase, you've learned that your market price is closer to 9.99€ than 10€."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Building your experience\n",
    "\n",
    "**Nb**: here, we will use $\\bar{x}$ the average conversion for a sample and $\\mu$ the average conversion for a population.\n",
    "\n",
    "#### General Principle\n",
    "\n",
    "Now that you know what an A/B test is, how would you perform one? The process remains the same as any Hypothesis Test:\n",
    "\n",
    "1. Define $H_0$ & $H_1$ \n",
    "2. Define a confidence level $\\alpha$\n",
    "3. Calculate Z-Score \n",
    "4. Deduct p-value \n",
    "5. Compare p-value to confidence level \n",
    "6. Conclude on test \n",
    "\n",
    "Only specifically for A/B test, you will consider the following: \n",
    "\n",
    "* $H_0$: A-B = 0 \n",
    "* $H_1$: A-B < 0 OR A-B > 0 (depending on your sample results)\n",
    "\n",
    "This means that :\n",
    "\n",
    "* The average conversion for A minus the average conversion for B is equal to 0, which would mean that your variation **has no significative impact** compared to your control variable.\n",
    "\n",
    "OR\n",
    "\n",
    "* The average conversion for A minus the average conversion for B is either greater or less than 0, which would mean that **your variation has significative impact** compared to your control variable.\n",
    "\n",
    "Now finally, your Z-Score formula will be a little different: \n",
    "\n",
    "$$Z-Score = \\frac{\\bar{x}_B - \\bar{x}_A}{\\sqrt{\\frac{\\sigma_B^2}{n_B}+ \\frac{\\sigma_A^2}{n_A}}}$$\n",
    "\n",
    "Where: \n",
    "\n",
    "* $\\bar{x}_A$: is the average conversion for A \n",
    "* $\\bar{x}_B$: is the average conversion for B\n",
    "* $\\sigma_A^2$: is the variation for A\n",
    "* $\\sigma_B^2$: is the variation for B\n",
    "* $n_A$: is the sample size for A\n",
    "* $n_B$: is the sample size for B\n",
    "\n",
    "**Nb**: the sample size should be above 30.\n",
    "\n",
    "👋 It is beyond the scope of this course but if you want to know why $Z-Score$ formula is a bit different, it comes from the fact that we are substracting two Normal Distributions. Feel free to check out this [short article from Wolfram Math World](https://mathworld.wolfram.com/NormalDifferenceDistribution.html) if you want to learn more. \n",
    "\n",
    "\n",
    "#### What to do when the results are inconclusive?\n",
    "\n",
    "If your experiement is inconclusive, you may decide to change your variations to make them more visible. For example, instead of simply changing the color of a button, you can also change its shape. This might make your users really see the difference and therefore convert more on one side than the other.\n",
    "\n",
    "You can also stop the experience and restart it with a variation on which you are sure to have a high traffic plan. For example, instead of having a variation on the checkout button, you can test a variation on the \"add to cart\" button because you know that more people click on it.\n",
    "\n",
    "## Last considerations before launching your A/B test\n",
    "\n",
    "We wanted to list here three points which are not intrinsically part of the A/B test but which are nevertheless very important to keep in mind during your experiments.\n",
    "\n",
    "### Statistical Significance vs. Practical Significance\n",
    "\n",
    "Now you know what statistical significance means. However, imagine that your experience has just statistically proven that you have improved your conversion rate by 0.001%. You've just wasted a lot of time and resources and ended up with an extremely minor and therefore unnecessary optimization. This is why you have to take into account what is called practical significance. That is to say, try to be as efficient as possible based not only on mathematics but also on your time and resource constraints.\n",
    "\n",
    "### Beware of Downside Metric\n",
    "\n",
    "When you run an experiment, you will sometimes find that some of your metrics improve but others decrease unexpectedly. For example, you may have improved your income but completely destroyed your commitment. This would mean that your users are buying but are no longer spending much time on your site.\n",
    "\n",
    "Depending on your goals, you will know if this is serious or not. The key is to be aware that other metrics may be negatively affected by your experience.\n",
    "\n",
    "### Local Maxima\n",
    "\n",
    "An A/B test is very useful to optimize your site incrementally. As you optimize your site, you will reach the maximum number of conversions your site can get with the design you have given it. This is called a local maximum. This does not mean that it is the maximum possible conversions in the market (or global maximum). If you do a complete redesign of your site, you may be able to get better results than what you had after all your optimizations.\n",
    "\n",
    "What we advise you to do is to optimize your current design until you feel that your optimizations have less and less impact on your metrics. Once you feel that you are reaching the local maximum, it means that you will have to try a new design of your site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources 📚📚\n",
    "\n",
    "- Complete Guide to A/B Testing - [https://bit.ly/2pNeOpl](https://bit.ly/2pNeOpl)\n",
    "- The Art of A/B Testing - [https://bit.ly/2SDJASs](https://towardsdatascience.com/the-art-of-a-b-testing-5a10c9bb70a4)\n",
    "- Test A/B: 10 mistakes to avoid [https://bit.ly/2GTGLTN](https://bit.ly/2GTGLTN)\n",
    "- Foundation of A/B Testing - [http://bit.ly/2p5t4rN](http://bit.ly/2p5t4rN)\n",
    "- What is A/B Testing - [http://bit.ly/1WgeMCj](http://bit.ly/1WgeMCj)\n",
    "- Intro to A/B Testing by Ever's Senior PM - [http://bit.ly/2sfCIbx](http://bit.ly/2sfCIbx)\n",
    "- A/B Testing with Yammer's Product Manager - [http://bit.ly/2sUQdPK](http://bit.ly/2sUQdPK)\n",
    "- \"Ask me Anything\" with Senior Product Manager at Optimizely - [http://bit.ly/2teoU5K](http://bit.ly/2teoU5K)\n",
    "- A/B Testing Basics: Confidence Interval - [https//bit.ly/43Zcs](https://www.crazyegg.com/blog/glossary/confidence-interval/#targetText=A%2FB%20Testing%20Basics%3A%20Confidence%20Interval&targetText=Confidence%20Interval%3A%20A%20range%20of,a%20parameter%20lies%20within%20it.&targetText=If%20you%20want%20to%20accurately,select%20a%20small%20confidence%20interval.)\n",
    "- Mobile A/B Testing Results Analysis: Statistical Significance, Confidence Level and Intervals - [https://bit.ly/3S2idx](https://splitmetrics.com/blog/mobile-a-b-testing-statistical-significance/)\n",
    "- Which Version Won? Understanding Confidence Level in Email A/B testing - [https://bit.ly/3Shei](https://www.act-on.com/blog/confidence-level-email-ab-testing/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

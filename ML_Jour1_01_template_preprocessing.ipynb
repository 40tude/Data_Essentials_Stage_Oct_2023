{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Template\n",
    "\n",
    "As you become a Machine Learning practioner, you will learn that preprocessing your data is key to building powerful algorithms. The cool thing though is that preprocessing very often follow the same steps. That is why we built a template for you that you can download and follow. \n",
    "\n",
    "## What you will learn in this course üßêüßê\n",
    "\n",
    "* Split your data into a train & a test set \n",
    "* Remplacing missing values in a dataset \n",
    "* Normalize your data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Import libraries üéí\n",
    "\n",
    "Usually, when preprocessing data, you will need: \n",
    "\n",
    "* `pandas`\n",
    "* `sklearn` \n",
    "\n",
    "As you will see in the code, when we are using huge librairies like `sklearn` we will want to only import part of the modules. For example, we will import only `train_test_split` that is part of `model_selection` module within `sklearn`. When you want to do that kind of action, you will need to follow this syntax: \n",
    "\n",
    "`from lib.module import function_or_class` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Import dataset üíæ\n",
    "\n",
    "Now let's import our dataset using `pandas`. As you already know, with this library you can import: \n",
    "\n",
    "* `.csv`\n",
    "* `.sql`\n",
    "* `.excel`\n",
    "* ... \n",
    "\n",
    "And almost any files and database you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "0   France  44.0  72000.0        No\n",
       "1    Spain  27.0  48000.0       Yes\n",
       "2  Germany  30.0  54000.0        No\n",
       "3    Spain  38.0  61000.0        No\n",
       "4  Germany  40.0      NaN       Yes"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import & visualize dataset\n",
    "df = pd.read_csv(\"assets/ML/Data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On veut savoir si les gesn ont achet√©\n",
    "* On est dans un pb de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4)\n",
      "       Country        Age        Salary Purchased\n",
      "count       10   9.000000      9.000000        10\n",
      "unique       3        NaN           NaN         2\n",
      "top     France        NaN           NaN        No\n",
      "freq         4        NaN           NaN         5\n",
      "mean       NaN  38.777778  63777.777778       NaN\n",
      "std        NaN   7.693793  12265.579662       NaN\n",
      "min        NaN  27.000000  48000.000000       NaN\n",
      "25%        NaN  35.000000  54000.000000       NaN\n",
      "50%        NaN  38.000000  61000.000000       NaN\n",
      "75%        NaN  44.000000  72000.000000       NaN\n",
      "max        NaN  50.000000  83000.000000       NaN\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of dataset in the form of (#rows, #columns)\n",
    "print(df.shape)\n",
    "\n",
    "# Describe dataset's main statistics\n",
    "print(df.describe(include=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Y a des valeurs manquantes pour toutes les colonnes\n",
    "* count est pas identique partout\n",
    "* 3 cat√©gories uniques de pays"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Separate Target from feature variables üéØ\n",
    "\n",
    "In this step, what you want to do is to **seperate your target variables from the ones that will be used to train your model**. Usually, we call `X` your features variable and `y` your target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating labels from features...\n",
      "...Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Separate target variable y from features X\n",
    "print(\"Separating labels from features...\")\n",
    "features_list = [\"Country\", \"Age\", \"Salary\"]\n",
    "X = df.loc[:,features_list]\n",
    "\n",
    "y = df.loc[:,\"Purchased\"]\n",
    "\n",
    "\n",
    "# print(X.head())\n",
    "# print(y.head())\n",
    "print(\"...Done.\")\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëã You could have used `iloc` as well. Whatever is handy for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_prime = df.iloc[:,[0,1,2]]\n",
    "# y_prime = df.iloc[:,[3]]\n",
    "# \n",
    "# print(X_prime.head())\n",
    "# print(y_prime.head())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Train / Test split üññ\n",
    "\n",
    "As you might have already guessed, once you trained your model, you will need some data to test it on. That's why you won't use your whole dataset for training. You will keep a small part for testing. That is why `train_test_split` from scikit-learn comes in handy. \n",
    "\n",
    "Usually we split into 80% training data and 20% testing but it can vary depending on how much data you can work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset into train set and test set...\n",
      "...Done.\n"
     ]
    }
   ],
   "source": [
    "# Divide dataset Train set & Test set \n",
    "## First we import train_test_split\n",
    "\n",
    "\n",
    "print(\"Splitting dataset into train set and test set...\")\n",
    "## Then we use train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=0,       # donne une valeur pour √™tre s√ªr d'avoir toujours le m√™me comportement random\n",
    "                                                    stratify=y)           # Allows you to stratify your sample. \n",
    "                                                                          # Meaning, you will have the same\n",
    "                                                                          # proportion of categories in test \n",
    "                                                                          # and train set\n",
    "\n",
    "print(\"...Done.\")                      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Cleaning ‚ûø\n",
    "\n",
    "Now we enter in the training step of the process. We will need to perform the following actions: \n",
    "\n",
    "1. Remplace missing values\n",
    "2. Encode categorical variables \n",
    "3. Standardize numerical features\n",
    "\n",
    "Let's tackle it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Training pipeline ###\n",
    "# print(\"--- Training pipeline ---\")\n",
    "# print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's deal with missing values. The most common way to deal with them is by replacing them with the mean of the corresponding column. We can do that with `SimpleImputer` class from `sklearn`. \n",
    "\n",
    "Si il avait manqu√© un nom de pays, on aurait pu utiiser une strategie \"mode\" pour remplacer par le pays le plus fr√©quent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing missing values...\n",
      "   Country   Age   Salary\n",
      "0   France  44.0  72000.0\n",
      "4  Germany  40.0      NaN\n",
      "6    Spain   NaN  52000.0\n",
      "9   France  37.0  67000.0\n",
      "3    Spain  38.0  61000.0\n",
      "1    Spain  27.0  48000.0\n",
      "2  Germany  30.0  54000.0\n",
      "5   France  35.0  58000.0\n",
      "\n",
      "...Done!\n",
      "\n",
      "   Country        Age        Salary\n",
      "0   France  44.000000  72000.000000\n",
      "4  Germany  40.000000  58857.142857\n",
      "6    Spain  35.857143  52000.000000\n",
      "9   France  37.000000  67000.000000\n",
      "3    Spain  38.000000  61000.000000\n",
      "1    Spain  27.000000  48000.000000\n",
      "2  Germany  30.000000  54000.000000\n",
      "5   France  35.000000  58000.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Missing values\n",
    "print(\"Imputing missing values...\")\n",
    "print(X_train)\n",
    "print()\n",
    "imputer = SimpleImputer(strategy=\"mean\") # Instanciate class of SimpleImputer with strategy of mean\n",
    "X_train = X_train.copy()                 # Copy dataset to avoid caveats of assign a copy of a slice of a DataFrame\n",
    "                                         # More info here https://towardsdatascience.com/explaining-the-settingwithcopywarning-in-pandas-ebc19d799d25\n",
    "\n",
    "X_train.iloc[:,[1,2]] = imputer.fit_transform(X_train.iloc[:,[1,2]]) # Fit and transform columns where there are missing values\n",
    "print(\"...Done!\\n\")\n",
    "print(X_train) \n",
    "print()   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëã NB: There are other statistics you can replace missing values with like *median*, *mode* or anything else as long as it is coherent with your business and your goals.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to do three things:\n",
    "\n",
    "1. Standardize data\n",
    "2. **One hot encode** categorical variables \n",
    "3. Encode labels of `y`\n",
    "\n",
    "The first one is because we need to stage data at the same scale. To do so, we say we *standardize* data. This means that we are going to remove the mean and divide by the standard deviation for each data point: \n",
    "\n",
    "$$Standard\\ Scaler = \\frac{x_i - \\mu}{\\sigma}$$ \n",
    "\n",
    "\n",
    "Le standard scaler remplace les valeurs par leur Z score\n",
    "\n",
    "```\n",
    "Country France All Angleterre\n",
    "France  1      0   0\n",
    "All     0      1   0\n",
    "Angl    0      0   1\n",
    "France  1      0   0\n",
    "```\n",
    "\n",
    "La 3eme colonne est redondante (drop) car si c'est pas l'allemange ni le france c'est forc√©ment UK\n",
    "```\n",
    "Country France All \n",
    "France  1      0 \n",
    "All     0      1 \n",
    "Angl    0      0 \n",
    "France  1      0 \n",
    "```\n",
    "\n",
    "Where: \n",
    "\n",
    "* $x_i$ is a given observation \n",
    "* $\\mu$ is the sample mean \n",
    "* $\\sigma$ is the sample standard deviation \n",
    "\n",
    "The second part is about encoding categorical variables. Indeed, simply replacing categories by a number is not enough because we need to make sure that each category weights the same. I.e if you replace \"cat\", \"dog\", \"rabbit\" by \"1\", \"2\", \"3\", then mathematically \"rabbit\" weights three times more than \"cat\". Thereore we will create a new column per category that can contain only `0` and `1`. That is what is called *one hot encoding*. \n",
    "\n",
    "On fait OneHot encoder quand y a pas de valeur entre les variables : france, all, spain\n",
    "On peut faire du labeling encoding (0, 1, 2...) si les cat√©gories sont par exemple : mauvais, moyen, bon\n",
    "\n",
    "\n",
    "Finally, for `y` we simply need to encode labels. Meaning we will replace \"yes\" / \"no\" by `0` and `1` which can be interpreted by a computer. \n",
    "\n",
    "\n",
    "Voir que dans le code on utulise un **ColumnTransformer()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing numeric features and encoding categorical features...\n",
      "\n",
      "...Done.\n",
      "[[ 0.00000000e+00  0.00000000e+00  1.61706195e+00  1.78674463e+00]\n",
      " [ 1.00000000e+00  0.00000000e+00  8.22715727e-01  0.00000000e+00]\n",
      " [ 0.00000000e+00  1.00000000e+00 -1.41104234e-15 -9.32214592e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  2.26956063e-01  1.10700483e+00]\n",
      " [ 0.00000000e+00  1.00000000e+00  4.25542617e-01  2.91317060e-01]]\n",
      "\n",
      "Encoding labels...\n",
      "0     No\n",
      "4    Yes\n",
      "6     No\n",
      "9    Yes\n",
      "3     No\n",
      "1    Yes\n",
      "2     No\n",
      "5    Yes\n",
      "Name: Purchased, dtype: object\n",
      "\n",
      "...Done.\n",
      "[0 1 0 1 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# X_train.iloc[:, 0].values.reshape(-1, 1)\n",
    "# country_transformed = ohe.fit_transform(X_train.iloc[:,0].values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "# Standardizing numeric features and encoding categorical features\n",
    "print(\"Standardizing numeric features and encoding categorical features...\")\n",
    "print()\n",
    "\n",
    "numeric_features = [1, 2]                             # On cr√©e une liste avec les indices des colonnes qui contiennent des valeurs num√©riques\n",
    "                                                      # Age et Salaires sont dans les colonnes 1 et 2\n",
    "numeric_transformer = StandardScaler()                # On pr√©cise le type de transformer qu'on veut utiliser pour les val num√©riques\n",
    "\n",
    "categorical_features = [0]                            # On cr√©e une liste avec les indices des colonnes qui contiennent des valeurs cat√©gorielles\n",
    "                                                      # Les pays sont dans la colonne d'indice 0 (la premi√®re)\n",
    "categorical_transformer = OneHotEncoder(drop='first') # Pour virer l'Angleterre comme dans l'exemple ci-dessus on aurait mis drop=\"last\" mais √ßa n'existe pas\n",
    "\n",
    "featureencoder = ColumnTransformer(                   # ColumnTransformer provient du module compose\n",
    "    transformers=[                                    # On passe une liste de transformers √† qui ont donne un nom (cat, num...)\n",
    "        ('cat', categorical_transformer, categorical_features),   \n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# La variable featureencoder est un object de type ColumnTransformer\n",
    "# Elle contient la \"recette\" pour transformer chacune des colonnes\n",
    "# Sur les colonnes 1 et 2 qui sont de type num√©rique appliquer StandarScaler\n",
    "# Sur la colonne 0 qui est de type cat√©gorielle, appliquer OneHotEncoder\n",
    "# ... \n",
    "# L'√©norme avantage de proc√©der comme √ßa c'est que \n",
    "#     si on veut tester un ou ajouter des transformers sur des colonnes on peut le faire en modifiant le code √† un seul endroit\n",
    "#     on est s√ªr d'appliquer la m√™me \"recette\" plus tard √† nos donn√©es de test (X_test)\n",
    "\n",
    "X_train = featureencoder.fit_transform(X_train)\n",
    "print(\"...Done.\")\n",
    "print(X_train[:5])  # print first 5 rows (not using iloc since now X_train became a numpy array)\n",
    "                    # ! X_train became a numpy array\n",
    "print()\n",
    "\n",
    "# On a 4 colonnes √† l'affichage car il y a 2 pays, age et salary\n",
    "# France => 0 dans les 2\n",
    "# All => 1 et 0\n",
    "# Spain => 0 et 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Encoding labels\n",
    "print(\"Encoding labels...\")\n",
    "print(y_train)\n",
    "print()\n",
    "labelencoder = LabelEncoder()                       # LabelEncoder provient de sklearn.preprocessing\n",
    "                                                    # Va transformer les Yes, No en 0, 1\n",
    "                                                    # Si on avait eu Riri, Fifi, Loulou en lables diff√©rents\n",
    "                                                    # il aurait cod√© en 0, 1 et 2\n",
    "Y_train = labelencoder.fit_transform(y_train)\n",
    "print(\"...Done.\")\n",
    "print(Y_train[:5])                                  # print first 5 rows (not using iloc since now y_train became a numpy array)\n",
    "                                                    # ! X_train became a numpy array\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are done and you should be able to train your model ! ü§ó As this is only the data preprocessing template, we won't cover the algorithm here. But simply remember that this would be the moment where you'll train your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** HERE WILL BE THE TRAINING STEP (NOT IN THE SCOPE AT THIS STAGE OF THE LECTURE) ***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"*** HERE WILL BE THE TRAINING STEP (NOT IN THE SCOPE AT THIS STAGE OF\" +\n",
    "                                           \" THE LECTURE) ***\")\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Testing üß™\n",
    "\n",
    "Finally, whatever preprocessing you did for your train set, you need to do it for your test set. Therefore here is how you can apply it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Test pipeline ---\n",
      "Imputing missing values...\n",
      "   Country   Age   Salary\n",
      "8  Germany  50.0  83000.0\n",
      "7   France  48.0  79000.0\n",
      "\n",
      "...Done.\n",
      "   Country   Age   Salary\n",
      "8  Germany  50.0  83000.0\n",
      "7   France  48.0  79000.0\n",
      "\n",
      "Encoding categorical features and standardizing numerical features...\n",
      "\n",
      "...Done.\n",
      "[[1.         0.         2.80858127 3.28217221]\n",
      " [0.         0.         2.41140816 2.73838036]]\n",
      "\n",
      "Encoding labels...\n",
      "8     No\n",
      "7    Yes\n",
      "Name: Purchased, dtype: object\n",
      "\n",
      "...Done.\n",
      "[0 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Test pipeline ###\n",
    "print(\"--- Test pipeline ---\")\n",
    "\n",
    "# Missing values\n",
    "print(\"Imputing missing values...\")\n",
    "print(X_test)\n",
    "print()\n",
    "X_test = X_test.copy() # Copy dataset to avoid caveats of assign a copy of a slice of a DataFrame\n",
    "                        # More info here https://towardsdatascience.com/explaining-the-settingwithcopywarning-in-pandas-ebc19d799d25\n",
    "\n",
    "X_test.iloc[:,[1,2]] = imputer.transform(X_test.iloc[:,[1,2]])\n",
    "print(\"...Done.\")\n",
    "print(X_test) \n",
    "print()   \n",
    "\n",
    "# Encoding categorical features and standardizing numeric features\n",
    "print(\"Encoding categorical features and standardizing numerical features...\")\n",
    "print()\n",
    "\n",
    "X_test = featureencoder.transform(X_test)       # Voir qu'on utilise la \"recette\" contenue dans l'objet featureencoder\n",
    "                                                # On est s√ªr et certains de traiter les donn√©es de test de la m√™me fa√ßon que les donn√©es de training  \n",
    "print(\"...Done.\")\n",
    "print(X_test)\n",
    "print()\n",
    "\n",
    "# Encoding labels\n",
    "print(\"Encoding labels...\")\n",
    "print(y_test)\n",
    "print()\n",
    "y_test = labelencoder.transform(y_test)\n",
    "print(\"...Done.\")\n",
    "print(y_test)\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Predict and evaluate üîÆ\n",
    "\n",
    "Now that your model is ready and trained, you can test its performance on your test set and interpret results! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** HERE WILL BE THE PREDICTION STEP ***\n",
      "\n",
      "*** HERE WILL BE THE ASSESSMENT OF PERFORMANCES ***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"*** HERE WILL BE THE PREDICTION STEP ***\")\n",
    "print()\n",
    "print(\"*** HERE WILL BE THE ASSESSMENT OF PERFORMANCES ***\")\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources üìöüìö\n",
    "\n",
    "* Standardization, or mean removal and variance scaling - [https://bit.ly/301Sx](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling)\n",
    "\n",
    "* Imputation of missing values - [https://bit.ly/02s0C](https://scikit-learn.org/stable/modules/impute.html)\n",
    "\n",
    "* Label encoding - [https://bit.ly/0Zasc](https://scikit-learn.org/stable/modules/preprocessing_targets.html#label-encoding)\n",
    "\n",
    "* Pipelines and composite estimators - [https://bit.ly/0csas2](https://scikit-learn.org/stable/modules/compose.html)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "216d08ced86f1f6e0b5764233bcb18334be12ba95b6ee555f60be9cf0be8c147"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
